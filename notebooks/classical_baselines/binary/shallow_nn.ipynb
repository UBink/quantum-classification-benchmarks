{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d21cc-550b-4b90-a423-88511755c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "path = \"../../data/binary/processed/\"\n",
    "\n",
    "datasets = [\"mnist_01_pca_4\",\n",
    "            \"mnist_01_pca_8\",\n",
    "            \"mnist_38_pca_4\",\n",
    "            \"mnist_38_pca_8\"]\n",
    "\n",
    "seeds = [42, 100, 20, 5, 99]\n",
    "sample_sizes = [500, 2000, 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d99dfc-c11f-4014-9ac6-3be6c8dcf331",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=2):\n",
    "        super(ShallowNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Calculate total parameters\n",
    "        self.total_params = sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_param_count(self):\n",
    "        return self.total_params\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs, device):\n",
    "    \"\"\"Train the neural network.\"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def evaluate_model(model, X, y, device):\n",
    "    \"\"\"Evaluate the model and return predictions.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X).to(device)\n",
    "        outputs = model(X_tensor)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def remap_labels(y):\n",
    "    \"\"\"\n",
    "    Remap labels to [0, 1] for binary classification.\n",
    "    This is necessary for PyTorch CrossEntropyLoss.\n",
    "    \n",
    "    For example:\n",
    "    - [0, 1] -> [0, 1] (no change)\n",
    "    - [3, 8] -> [0, 1]\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(y)\n",
    "    if len(unique_labels) != 2:\n",
    "        raise ValueError(f\"Expected 2 unique labels, got {len(unique_labels)}: {unique_labels}\")\n",
    "    \n",
    "    # Create mapping: first unique label -> 0, second -> 1\n",
    "    label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "    return np.array([label_map[label] for label in y])\n",
    "\n",
    "\n",
    "# Hyperparameter search space\n",
    "hidden_dims = [8, 16, 32]  # Will give ~50-100 params depending on input_dim\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [32, 64]\n",
    "epochs_options = [50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33134bb8-7736-4d57-a9a1-345d701f9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "results_path = \"../../results/classical_nn_baseline_results.json\"\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, 'r') as f:\n",
    "        all_results = json.load(f)\n",
    "    print(f\"Loaded existing results with {len(all_results['results'])} experiments\")\n",
    "else:\n",
    "    all_results = {\n",
    "        \"experiment_info\": {\n",
    "            \"model_type\": \"shallow_neural_network\",\n",
    "            \"date\": datetime.now().isoformat(),\n",
    "            \"architecture\": \"Single hidden layer with ReLU activation\",\n",
    "            \"hyperparameter_tuning\": \"Grid search over hidden_dim, learning_rate, batch_size, epochs\",\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"device\": str(device)\n",
    "        },\n",
    "        \"results\": []\n",
    "    }\n",
    "    print(\"Created new results file\")\n",
    "\n",
    "# Main experiment loop\n",
    "for dataset in datasets:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    \n",
    "    # Load data\n",
    "    X_full = np.load(os.path.join(path, f\"{dataset}/X_train.npy\"))\n",
    "    y_full = np.load(os.path.join(path, f\"{dataset}/y_train.npy\"))\n",
    "    X_test = np.load(os.path.join(path, f\"{dataset}/X_test.npy\"))\n",
    "    y_test = np.load(os.path.join(path, f\"{dataset}/y_test.npy\"))\n",
    "    \n",
    "    \n",
    "    y_full = remap_labels(y_full)\n",
    "    y_test = remap_labels(y_test)\n",
    "   \n",
    "    \n",
    "    print(f\"Available training samples: {X_full.shape[0]}\")\n",
    "    print(f\"Input dimension: {X_full.shape[1]}\")\n",
    "    print(f\"Unique labels in training: {np.unique(y_full)}\")  # Should be [0, 1]\n",
    "    print(f\"Unique labels in test: {np.unique(y_test)}\")      # Should be [0, 1]\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    input_dim = X_full.shape[1]\n",
    "    \n",
    "    # Test different sample sizes\n",
    "    for n_samples in sample_sizes:\n",
    "        if n_samples > X_full.shape[0]:\n",
    "            n_samples = X_full.shape[0]\n",
    "        \n",
    "        print(f\"{'─'*70}\")\n",
    "        if n_samples == X_full.shape[0]:\n",
    "            print(f\"Training with FULL dataset ({n_samples} samples)\")\n",
    "        else:\n",
    "            print(f\"Training with {n_samples} samples (subsampled)\")\n",
    "        print(f\"{'─'*70}\")\n",
    "        \n",
    "        for seed in seeds:\n",
    "            \n",
    "            experiment_exists = any(\n",
    "                r[\"dataset\"] == dataset and \n",
    "                r[\"n_train\"] == n_samples and \n",
    "                r[\"seed\"] == seed\n",
    "                for r in all_results[\"results\"]\n",
    "            )\n",
    "            \n",
    "            if experiment_exists:\n",
    "                print(f\"  Seed {seed:3d}: SKIPPING (already exists)\")\n",
    "                continue\n",
    "            \n",
    "            # Subsample if needed\n",
    "            if n_samples < X_full.shape[0]:\n",
    "                indices = np.random.RandomState(seed).choice(\n",
    "                    X_full.shape[0], n_samples, replace=False\n",
    "                )\n",
    "                X_train = X_full[indices]\n",
    "                y_train = y_full[indices]\n",
    "            else:\n",
    "                X_train = X_full\n",
    "                y_train = y_full\n",
    "            \n",
    "            # Split into train/validation (80/20)\n",
    "            X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "                X_train, y_train, test_size=0.2, random_state=seed, stratify=y_train\n",
    "            )\n",
    "            \n",
    "            # Set random seeds for reproducibility\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed(seed)\n",
    "            \n",
    "            # Hyperparameter tuning via grid search on validation set\n",
    "            best_val_acc = 0\n",
    "            best_params = {}\n",
    "            \n",
    "            for hidden_dim in hidden_dims:\n",
    "                for lr in learning_rates:\n",
    "                    for batch_size in batch_sizes:\n",
    "                        for epochs in epochs_options:\n",
    "                            # Create model\n",
    "                            model = ShallowNN(input_dim, hidden_dim).to(device)\n",
    "                            \n",
    "                            # Skip if parameter count is too far from target (50-100)\n",
    "                            param_count = model.get_param_count()\n",
    "                            if param_count > 150:  # Allow some flexibility\n",
    "                                continue\n",
    "                            \n",
    "                            # Create data loaders\n",
    "                            train_dataset = TensorDataset(\n",
    "                                torch.FloatTensor(X_train_split),\n",
    "                                torch.LongTensor(y_train_split)\n",
    "                            )\n",
    "                            train_loader = DataLoader(\n",
    "                                train_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True\n",
    "                            )\n",
    "                            \n",
    "                            # Train\n",
    "                            criterion = nn.CrossEntropyLoss()\n",
    "                            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                            train_model(model, train_loader, criterion, optimizer, epochs, device)\n",
    "                            \n",
    "                            # Validate\n",
    "                            y_val_pred = evaluate_model(model, X_val, y_val, device)\n",
    "                            val_acc = accuracy_score(y_val, y_val_pred)\n",
    "                            \n",
    "                            # Track best\n",
    "                            if val_acc > best_val_acc:\n",
    "                                best_val_acc = val_acc\n",
    "                                best_params = {\n",
    "                                    'hidden_dim': hidden_dim,\n",
    "                                    'learning_rate': lr,\n",
    "                                    'batch_size': batch_size,\n",
    "                                    'epochs': epochs,\n",
    "                                    'param_count': param_count\n",
    "                                }\n",
    "            \n",
    "            # Train final model with best hyperparameters on full training set\n",
    "            print(f\"  Seed {seed:3d}: Best params - hidden={best_params['hidden_dim']}, \"\n",
    "                  f\"lr={best_params['learning_rate']}, batch={best_params['batch_size']}, \"\n",
    "                  f\"epochs={best_params['epochs']}, params={best_params['param_count']}\")\n",
    "            \n",
    "            # Set seeds again for final training\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed(seed)\n",
    "            \n",
    "            final_model = ShallowNN(input_dim, best_params['hidden_dim']).to(device)\n",
    "            \n",
    "            # Create data loader for full training set\n",
    "            train_dataset = TensorDataset(\n",
    "                torch.FloatTensor(X_train),\n",
    "                torch.LongTensor(y_train)\n",
    "            )\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=best_params['batch_size'],\n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(final_model.parameters(), lr=best_params['learning_rate'])\n",
    "            \n",
    "            # Training time\n",
    "            start_time = time.time()\n",
    "            train_model(final_model, train_loader, criterion, optimizer, \n",
    "                       best_params['epochs'], device)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Inference time\n",
    "            start_time = time.time()\n",
    "            y_pred = evaluate_model(final_model, X_test, y_test, device)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "            \n",
    "            result = {\n",
    "                \"dataset\": dataset,\n",
    "                \"n_train\": int(n_samples),\n",
    "                \"n_test\": int(X_test.shape[0]),\n",
    "                \"n_features\": int(X_train.shape[1]),\n",
    "                \"seed\": int(seed),\n",
    "                \"best_hidden_dim\": int(best_params['hidden_dim']),\n",
    "                \"best_learning_rate\": float(best_params['learning_rate']),\n",
    "                \"best_batch_size\": int(best_params['batch_size']),\n",
    "                \"best_epochs\": int(best_params['epochs']),\n",
    "                \"param_count\": int(best_params['param_count']),\n",
    "                \"accuracy\": float(accuracy),\n",
    "                \"f1_score\": float(f1),\n",
    "                \"training_time_seconds\": float(training_time),\n",
    "                \"inference_time_seconds\": float(inference_time),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            all_results[\"results\"].append(result)\n",
    "            \n",
    "            print(f\"            Test - Acc={accuracy:.4f}, F1={f1:.4f}, \"\n",
    "                  f\"Train={training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2055ad45-f243-4a6a-bcc2-545892d83379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "os.makedirs(\"../../results\", exist_ok=True)\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(all_results, indent=2, fp=f)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Results saved to {results_path}\")\n",
    "print(f\"Total experiments: {len(all_results['results'])}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c4fe1-662e-48aa-a3d7-401b48be8308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
