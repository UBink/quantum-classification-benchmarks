{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e43822d-0fcc-4b68-a3f1-0b68e95a1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "path = \"../../data/binary/processed/\"\n",
    "\n",
    "datasets = [\"mnist_01_pca_4\",\n",
    "            \"mnist_01_pca_8\",\n",
    "            \"mnist_38_pca_4\",\n",
    "            \"mnist_38_pca_8\"]\n",
    "\n",
    "# Randomly selected numbers for 5 different seeds\n",
    "seeds = [42, 100, 20, 5, 99]\n",
    "\n",
    "sample_sizes = [500, 2000, 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce3fbd-3782-4554-adb5-a0088220e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing results or create new\n",
    "results_path = \"../../results/classical_logreg_baseline_results.json\"\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, 'r') as f:\n",
    "        all_results = json.load(f)\n",
    "    print(f\"Loaded existing results with {len(all_results['results'])} entries\")\n",
    "else:\n",
    "    all_results = {\n",
    "        \"experiment_info\": {\n",
    "            \"model_type\": \"classical_logistic_regression\",\n",
    "            \"date\": datetime.now().isoformat(),\n",
    "            \"hyperparameter_tuning\": \"GridSearchCV with C=[0.001, 0.01, 0.1, 1, 10, 100]\",\n",
    "            \"cv_folds\": 5\n",
    "        },\n",
    "        \"results\": []\n",
    "    }\n",
    "    print(\"Created new results file\")\n",
    "\n",
    "# Run experiments\n",
    "for dataset in datasets:\n",
    "    dataset_path = path + dataset\n",
    "    \n",
    "    # Load full training data\n",
    "    X_train_full = np.load(dataset_path + \"/X_train.npy\")\n",
    "    X_test = np.load(dataset_path + \"/X_test.npy\")\n",
    "    y_train_full = np.load(dataset_path + \"/y_train.npy\")\n",
    "    y_test = np.load(dataset_path + \"/y_test.npy\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    print(f\"Available training samples: {X_train_full.shape[0]}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for n_samples in sample_sizes:\n",
    "        # Skip if already exists\n",
    "        existing = [r for r in all_results[\"results\"] \n",
    "                   if r[\"dataset\"] == dataset and r[\"n_train\"] == n_samples]\n",
    "        \n",
    "        if existing and len(existing) >= len(seeds):\n",
    "            print(f\"\\n Skipping {dataset} with {n_samples} samples (already complete)\")\n",
    "            continue\n",
    "        \n",
    "        # Skip if requesting more than available\n",
    "        if n_samples > X_train_full.shape[0]:\n",
    "            print(f\"\\n Skipping {n_samples} samples (only {X_train_full.shape[0]} available)\")\n",
    "            continue\n",
    "        \n",
    "        # Use full dataset or subsample\n",
    "        if n_samples == X_train_full.shape[0]:\n",
    "            X_train, y_train = X_train_full, y_train_full\n",
    "            print(f\"\\n{'─'*70}\")\n",
    "            print(f\"Training with FULL dataset ({n_samples} samples)\")\n",
    "            print(f\"{'─'*70}\")\n",
    "        else:\n",
    "            X_train, _, y_train, _ = train_test_split(\n",
    "                X_train_full, y_train_full,\n",
    "                train_size=n_samples,\n",
    "                random_state=42,  # Fixed for consistency across runs\n",
    "                stratify=y_train_full\n",
    "            )\n",
    "            print(f\"\\n{'─'*70}\")\n",
    "            print(f\"Training with {n_samples} samples (subsampled)\")\n",
    "            print(f\"{'─'*70}\")\n",
    "        \n",
    "        # Run with different seeds for timing variation\n",
    "        for seed in seeds:\n",
    "            # Check if this specific experiment exists\n",
    "            specific_existing = [r for r in all_results[\"results\"] \n",
    "                                if r[\"dataset\"] == dataset \n",
    "                                and r[\"n_train\"] == n_samples \n",
    "                                and r.get(\"seed\") == seed]\n",
    "            \n",
    "            if specific_existing:\n",
    "                print(f\"  Seed {seed}: Already exists, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Hyperparameter tuning with GridSearchCV\n",
    "            param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "            log_reg_cv = GridSearchCV(\n",
    "                LogisticRegression(random_state=seed, max_iter=1000), \n",
    "                param_grid, \n",
    "                cv=5,\n",
    "                n_jobs=-1  # Use all CPU cores for faster training\n",
    "            )\n",
    "            \n",
    "            # Train\n",
    "            start_time = time.time()\n",
    "            log_reg_cv.fit(X_train, y_train)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Inference\n",
    "            start_time = time.time()\n",
    "            y_pred = log_reg_cv.predict(X_test)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                \"dataset\": dataset,\n",
    "                \"n_train\": int(n_samples),\n",
    "                \"n_test\": int(X_test.shape[0]),\n",
    "                \"n_features\": int(X_train.shape[1]),\n",
    "                \"seed\": int(seed),\n",
    "                \"best_C\": float(log_reg_cv.best_params_['C']),\n",
    "                \"accuracy\": float(accuracy),\n",
    "                \"f1_score\": float(f1),\n",
    "                \"training_time_seconds\": float(training_time),\n",
    "                \"inference_time_seconds\": float(inference_time),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            all_results[\"results\"].append(result)\n",
    "            \n",
    "            print(f\"  Seed {seed:3d}: Acc={accuracy:.4f}, F1={f1:.4f}, \"\n",
    "                  f\"Best C={log_reg_cv.best_params_['C']:.3f}, \"\n",
    "                  f\"Train={training_time:.3f}s\")\n",
    "\n",
    "# Save results\n",
    "os.makedirs(\"../../results\", exist_ok=True)\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(all_results, indent=2, fp=f)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Results saved to {results_path}\")\n",
    "print(f\"Total experiments: {len(all_results['results'])}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bcb99f-f7c3-4392-973f-78f8c12f6274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qml-env)",
   "language": "python",
   "name": "qml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
